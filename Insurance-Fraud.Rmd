---
title: "Cliam Fraud"
author: "BonnyFace Kalong"
date: "`r Sys.Date()`"
output: pData_document
---

```{r}
Data<-read.csv("./DataSet/fraud_oracle.csv")
head(Data)
```
```{r}
library(visdat)
vis_dat(Data)
```

```{r}
library(skimr)
skimmed <- skim_to_wide(Data)
skimmed[, c(1:5, 9:11, 13, 15:16)]
```
```{r}
y <- Data$FraudFound_P
```

```{r,warning=FALSE}
library(ggplot2)
library(gridExtra)

# Create a list of countplots
plots <- list(
  ggplot(Data, aes(x=Month, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    scale_x_discrete(limits=c('Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec')) + 
    labs(title="Month"),
  
  ggplot(Data, aes(x=DayOfWeek, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    scale_x_discrete(limits=c('Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday')) + 
    labs(title="Day"),
  
  ggplot(Data, aes(x=Sex, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    labs(title="Sex"),
  
  ggplot(Data, aes(x=MaritalStatus, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    labs(title="Marital Status"),
  
  ggplot(Data, aes(x=NumberOfCars, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    labs(title="Number Of Cars"),
  
  ggplot(Data, aes(x=AccidentArea, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    labs(title="Accident Area"),
  
  ggplot(Data, aes(x=DriverRating, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    labs(title="Driver Rating"),
  
  ggplot(Data, aes(x=AgentType, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    labs(title="Agent Type"),
  
  ggplot(Data, aes(x=BasePolicy, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    labs(title="Base Policy")
)

# Arrange the plots in a grid
grid.arrange(
  grobs = plots,
  ncol = 3,
  nrow = 3,
  top = "Countplots of Categorical Variables by Fraud Found",
  widths = c(10, 10, 10),
  heights = c(5, 5, 5)
)

```
```{r}
# library(ggplot2)
# library(gridExtra)
# df <- Data
# pdf("countplots.pdf", width = 15, height = 10)
# 
# p1 <- ggplot(df, aes(x=Month, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   scale_x_discrete(limits=c('Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec')) + 
#   labs(title="Month")
# 
# p2 <- ggplot(df, aes(x=DayOfWeek, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   scale_x_discrete(limits=c('Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday')) + 
#   labs(title="Day")
# 
# p3 <- ggplot(df, aes(x=Sex, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   labs(title="Sex")
# 
# p4 <- ggplot(df, aes(x=MaritalStatus, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   labs(title="Marital Status")
# 
# p5 <- ggplot(df, aes(x=NumberOfCars, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   labs(title="Number Of Cars")
# 
# p6 <- ggplot(df, aes(x=AccidentArea, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   labs(title="Accident Area")
# 
# p7 <- ggplot(df, aes(x=DriverRating, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   labs(title="Driver Rating")
# 
# p8 <- ggplot(df, aes(x=AgentType, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   labs(title="Agent Type")
# 
# p9 <- ggplot(df, aes(x=BasePolicy, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   labs(title="Base Policy")
# 
# grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, ncol=3, nrow=3, top="Countplots of Categorical Variables by Fraud Found")
# 
# dev.off()

```

```{r,warning=FALSE,message=FALSE message=FALSE, r,warning=FALSE}
library(tidyverse)
# find categorical variables
cat_vars <- sapply(Data, class) %>% 
  .[. != "numeric" & . != "integer" & . != "double"] %>% 
  names()

# count unique values in each categorical variable
cat_counts <- lapply(Data[, cat_vars], function(x) length(unique(x)))

# combine variable names and counts into a data frame
cat_summary <- data.frame(
  Variable = names(cat_counts),
  Unique_Count = unlist(cat_counts)
)

# print results
cat_summary
```
```{r}
library(metan)
plot(corr_coef(Data))
```
```{r}
non_collinear_vars(Data,max_vif = 5)
```
```{r}
Data<-Data%>%select(-Year)
```

```{r,warning=FALSE}
library(caret)
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(FraudFound_P ~ ., data=Data)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mat <- predict(dummies_model, newdata = Data)

# # Convert to dataframe
Data <- data.frame(trainData_mat)

# # See the structure of the new dataset
str(Data)
```
```{r}
# Normalize numerical variables (excluding target variable)
preprocessParams <- preProcess(Data, method = "BoxCox") #BoxCox: Remove skewness leading to normality. Values must be > 0
```

```{r}
# # Normalize numerical variables
# num_cols <- sapply(Data, is.numeric)
# num_data <- Data[, num_cols]
# # Normalize numerical variables (excluding target variable)
# preprocessParams <- preProcess(num_data %>% select(-FraudFound_P), method = "BoxCox") #BoxCox: Remove skewness leading to normality. Values must be > 0
# norm_num_data <- predict(preprocessParams, num_data)
# 
# # Combine encoded and normalized data frames
# processed_data <- cbind(norm_num_data, encoded_data)
# processed_data$FraudFound_P <- factor(processed_data$FraudFound_P , levels = c(0, 1))

```
```{r}
Data <- predict(preprocessParams, newdata = Data)
```
```{r}
# Append the Y variable
Data$FraudFound_P <- y

apply(Data[, 1:10], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})
```
```{r}
# set.seed(100)
# options(warn=-1)
# 
# subsets <- c(1:5, 10, 15, 18)
# 
# ctrl <- rfeControl(functions = rfFuncs,
#                    method = "repeatedcv",
#                    repeats = 2,
#                    verbose = FALSE)
# 
# lmProfile <- rfe(x=Data[, 1:148], y=Data$FraudFound_P,
#                  sizes = subsets,
#                  rfeControl = ctrl)
# 
# lmProfile
```

```{r}

```


```{r}
# Split data into training, validation, and testing sets
set.seed(123)  # for reproducibility
inTrain <- createDataPartition(Data$FraudFound_P, p = 0.7, list = FALSE)
training <- Data[inTrain, ]
remaining <- Data[-inTrain, ]
inTest <- createDataPartition(remaining$FraudFound_P, p = 0.5, list = FALSE)
validation <- remaining[inTest, ]
testing <- remaining[-inTest, ]
```

```{r}
formula <- as.formula("FraudFound_P ~ .")
```

```{r}
library(xgboost)

# Define the grid of hyperparameters to search
hyperparams <- expand.grid(
  max_depth = c(1, 2, 3, 4),
  eta = c(0.1, 0.3, 0.5),
  gamma = c(0, 0.2, 0.5),
  colsample_bytree = c(0.5, 0.8),
  min_child_weight = c(1, 3),
  subsample = c(0.5, 0.8),
  nrounds = c(50, 100, 200, 300)
)
```

```{r,warning=FALSE}
# Train and tune the XGBoost model using cross-validation
set.seed(123)
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     verboseIter = TRUE,
                     allowParallel = TRUE, 
                     classProbs = FALSE,
                     search = "random")
model1 <- train(
 factor( FraudFound_P) ~ .,
  data = training,
  trControl = ctrl,
  # tuneGrid = hyperparams,
  method = "xgbTree",
  metric = "AUC",
  tuneLength = 2, # number of combinations to try in hyperparameter tuning
  verbose = TRUE,
  objective = "binary:logistic", # specify objective for binary classification
  eval_metric = "auc" # specify evaluation metric for binary classification
)
```
```{r}

# Print the best hyperparameters and model performance
print(model1$bestTune)
print(model1$results)
print(model1$finalModel)

```
```{r}
print(model1)
```
```{r}
plot(model1)
```
```{r}
varImp(model1)
```

```{r}
plot(varImp(model1), main="Variable Importance")
```

```{r}
# Evaluate the model performance on the testing set
library(pROC)

# Make predictions on the validation set
pred_validation <- predict(model1, validation)
```
```{r}

# Compute and print the confusion matrix
confusionMatrix(reference = factor(validation$FraudFound_P), data = pred_validation, mode='everything', positive='1')
```

The confusion matrix shows that the XGBOOST model correctly predicted 2147 true negatives and 40 true positives. However, it incorrectly predicted 113 false negatives and 13 false positives.

The overall accuracy of the model is 0.9455, which means it correctly classified 94.55% of the observations in the testing set. The model has a sensitivity of 0.26144, which means it correctly identified 26.14% of the fraud cases in the testing set. The specificity of the model is 0.99398, which means it correctly identified 99.398% of the non-fraud cases in the testing set.

The precision of the model is 0.75472, which means out of all the cases predicted as fraud, 75.47% were actually fraud cases. The negative predictive value (NPV) of the model is 0.95, which means out of all the cases predicted as non-fraud, 95% were actually non-fraud cases. The NPV indicates the ability of the model to identify true negative cases.

The F1 score of the model is 0.38835, which is the harmonic mean of precision and recall. It is a measure of the tradeoff between precision and recall. In this case, the F1 score indicates that the model has a relatively low performance in terms of identifying fraud cases.

The balanced accuracy of the model is 0.62771, which is the average of sensitivity and specificity. It is a measure of how well the model performs on both positive and negative classes. In this case, the balanced accuracy indicates that the model has a relatively poor performance in terms of identifying fraud cases.


True negatives (TN) are the cases in which the actual class is negative (not fraud) and the model predicts negative (not fraud). In other words, TN represents the number of correctly classified negative cases. In the confusion matrix, TN is located at the top left corner.
```{r}
# Compute and print the AUC
auc <- roc(as.numeric(validation$FraudFound),as.numeric( pred_validation))$auc
cat("AUC:", auc, "\n")

# Plot the ROC curve
plot(roc(as.numeric(validation$FraudFound),as.numeric( pred_validation)), main = "ROC Curve", print.thres = c(0.1, 0.25, 0.5, 0.75, 0.9))


```

```{r}
# Plot the feature importance
xgb.importance(model = model1$finalModel) 

```

```{r}
# Train and tune the XGBoost model using cross-validation
set.seed(123)
ctrl <- trainControl(method = "cv", number = 10, verboseIter = TRUE, allowParallel = TRUE)
model2 <- train(
  factor(FraudFound_P )~ .,
  data = training,
  trControl = ctrl,
  tuneGrid = hyperparams,
  method = "xgbTree",
  metric = "AUC",
  tuneLength = 2, # number of combinations to try in hyperparameter tuning
  
  verbose = TRUE,
  objective = "binary:logistic", # specify objective for binary classification
  eval_metric = "auc" # specify evaluation metric for binary classification
)


# Print the best hyperparameters and model performance
model2
```







































