Data<-read.csv("./DataSet/fraud_oracle.csv")
head(Data)
library(visdat)
vis_dat(Data)
library(skimr)
skimmed <- skim_to_wide(Data)
skimmed[, c(1:5, 9:11, 13, 15:16)]
y <- Data$FraudFound_P
library(ggplot2)
library(gridExtra)
# Create a list of countplots
plots <- list(
ggplot(Data, aes(x=Month, fill=FraudFound_P)) +
geom_bar(position="dodge") +
scale_x_discrete(limits=c('Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec')) +
labs(title="Month"),
ggplot(Data, aes(x=DayOfWeek, fill=FraudFound_P)) +
geom_bar(position="dodge") +
scale_x_discrete(limits=c('Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday')) +
labs(title="Day"),
ggplot(Data, aes(x=Sex, fill=FraudFound_P)) +
geom_bar(position="dodge") +
labs(title="Sex"),
ggplot(Data, aes(x=MaritalStatus, fill=FraudFound_P)) +
geom_bar(position="dodge") +
labs(title="Marital Status"),
ggplot(Data, aes(x=NumberOfCars, fill=FraudFound_P)) +
geom_bar(position="dodge") +
labs(title="Number Of Cars"),
ggplot(Data, aes(x=AccidentArea, fill=FraudFound_P)) +
geom_bar(position="dodge") +
labs(title="Accident Area"),
ggplot(Data, aes(x=DriverRating, fill=FraudFound_P)) +
geom_bar(position="dodge") +
labs(title="Driver Rating"),
ggplot(Data, aes(x=AgentType, fill=FraudFound_P)) +
geom_bar(position="dodge") +
labs(title="Agent Type"),
ggplot(Data, aes(x=BasePolicy, fill=FraudFound_P)) +
geom_bar(position="dodge") +
labs(title="Base Policy")
)
# Arrange the plots in a grid
grid.arrange(
grobs = plots,
ncol = 3,
nrow = 3,
top = "Countplots of Categorical Variables by Fraud Found",
widths = c(10, 10, 10),
heights = c(5, 5, 5)
)
# library(ggplot2)
# library(gridExtra)
# df <- Data
# pdf("countplots.pdf", width = 15, height = 10)
#
# p1 <- ggplot(df, aes(x=Month, fill=FraudFound_P)) +
#   geom_bar(position="dodge") +
#   scale_x_discrete(limits=c('Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec')) +
#   labs(title="Month")
#
# p2 <- ggplot(df, aes(x=DayOfWeek, fill=FraudFound_P)) +
#   geom_bar(position="dodge") +
#   scale_x_discrete(limits=c('Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday')) +
#   labs(title="Day")
#
# p3 <- ggplot(df, aes(x=Sex, fill=FraudFound_P)) +
#   geom_bar(position="dodge") +
#   labs(title="Sex")
#
# p4 <- ggplot(df, aes(x=MaritalStatus, fill=FraudFound_P)) +
#   geom_bar(position="dodge") +
#   labs(title="Marital Status")
#
# p5 <- ggplot(df, aes(x=NumberOfCars, fill=FraudFound_P)) +
#   geom_bar(position="dodge") +
#   labs(title="Number Of Cars")
#
# p6 <- ggplot(df, aes(x=AccidentArea, fill=FraudFound_P)) +
#   geom_bar(position="dodge") +
#   labs(title="Accident Area")
#
# p7 <- ggplot(df, aes(x=DriverRating, fill=FraudFound_P)) +
#   geom_bar(position="dodge") +
#   labs(title="Driver Rating")
#
# p8 <- ggplot(df, aes(x=AgentType, fill=FraudFound_P)) +
#   geom_bar(position="dodge") +
#   labs(title="Agent Type")
#
# p9 <- ggplot(df, aes(x=BasePolicy, fill=FraudFound_P)) +
#   geom_bar(position="dodge") +
#   labs(title="Base Policy")
#
# grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, ncol=3, nrow=3, top="Countplots of Categorical Variables by Fraud Found")
#
# dev.off()
library(tidyverse)
# find categorical variables
cat_vars <- sapply(Data, class) %>%
.[. != "numeric" & . != "integer" & . != "double"] %>%
names()
# count unique values in each categorical variable
cat_counts <- lapply(Data[, cat_vars], function(x) length(unique(x)))
# combine variable names and counts into a data frame
cat_summary <- data.frame(
Variable = names(cat_counts),
Unique_Count = unlist(cat_counts)
)
# print results
cat_summary
library(metan)
plot(corr_coef(Data))
non_collinear_vars(Data,max_vif = 5)
Data<-Data%>%select(-Year)
library(caret)
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(FraudFound_P ~ ., data=Data)
# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mat <- predict(dummies_model, newdata = Data)
# # Convert to dataframe
Data <- data.frame(trainData_mat)
# # See the structure of the new dataset
str(Data)
# Normalize numerical variables (excluding target variable)
preprocessParams <- preProcess(Data, method = "BoxCox") #BoxCox: Remove skewness leading to normality. Values must be > 0
# # Normalize numerical variables
# num_cols <- sapply(Data, is.numeric)
# num_data <- Data[, num_cols]
# # Normalize numerical variables (excluding target variable)
# preprocessParams <- preProcess(num_data %>% select(-FraudFound_P), method = "BoxCox") #BoxCox: Remove skewness leading to normality. Values must be > 0
# norm_num_data <- predict(preprocessParams, num_data)
#
# # Combine encoded and normalized data frames
# processed_data <- cbind(norm_num_data, encoded_data)
# processed_data$FraudFound_P <- factor(processed_data$FraudFound_P , levels = c(0, 1))
Data <- predict(preprocessParams, newdata = Data)
# Append the Y variable
Data$FraudFound_P <- y
apply(Data[, 1:10], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})
# set.seed(100)
# options(warn=-1)
#
# subsets <- c(1:5, 10, 15, 18)
#
# ctrl <- rfeControl(functions = rfFuncs,
#                    method = "repeatedcv",
#                    repeats = 2,
#                    verbose = FALSE)
#
# lmProfile <- rfe(x=Data[, 1:148], y=Data$FraudFound_P,
#                  sizes = subsets,
#                  rfeControl = ctrl)
#
# lmProfile
# Split data into training, validation, and testing sets
set.seed(123)  # for reproducibility
inTrain <- createDataPartition(Data$FraudFound_P, p = 0.7, list = FALSE)
training <- Data[inTrain, ]
remaining <- Data[-inTrain, ]
inTest <- createDataPartition(remaining$FraudFound_P, p = 0.5, list = FALSE)
validation <- remaining[inTest, ]
testing <- remaining[-inTest, ]
formula <- as.formula("FraudFound_P ~ .")
library(xgboost)
# Define the grid of hyperparameters to search
hyperparams <- expand.grid(
max_depth = c(1, 2, 3, 4),
eta = c(0.1, 0.3, 0.5),
gamma = c(0, 0.2, 0.5),
colsample_bytree = c(0.5, 0.8),
min_child_weight = c(1, 3),
subsample = c(0.5, 0.8),
nrounds = c(50, 100, 200, 300)
)
# Train and tune the XGBoost model using cross-validation
set.seed(123)
ctrl <- trainControl(method = "repeatedcv",
number = 10,
verboseIter = TRUE,
allowParallel = TRUE,
classProbs = FALSE,
search = "random")
model1 <- train(
factor( FraudFound_P) ~ .,
data = training,
trControl = ctrl,
# tuneGrid = hyperparams,
method = "xgbTree",
metric = "AUC",
tuneLength = 2, # number of combinations to try in hyperparameter tuning
verbose = TRUE,
objective = "binary:logistic", # specify objective for binary classification
eval_metric = "auc" # specify evaluation metric for binary classification
)
# Print the best hyperparameters and model performance
print(model1$bestTune)
print(model1$results)
print(model1$finalModel)
print(model1)
plot(model1)
varImp(model1)
plot(varImp(model1), main="Variable Importance")
# Evaluate the model performance on the testing set
library(pROC)
# Make predictions on the validation set
pred_validation <- predict(model1, validation)
# Compute and print the confusion matrix
confusionMatrix(reference = factor(validation$FraudFound_P), data = pred_validation, mode='everything', positive='1')
# Compute and print the AUC
auc <- roc(as.numeric(validation$FraudFound),as.numeric( pred_validation))$auc
cat("AUC:", auc, "\n")
# Plot the ROC curve
plot(roc(as.numeric(validation$FraudFound),as.numeric( pred_validation)), main = "ROC Curve", print.thres = c(0.1, 0.25, 0.5, 0.75, 0.9))
# Plot the feature importance
xgb.importance(model = model1$finalModel)
# Train and tune the XGBoost model using cross-validation
set.seed(123)
ctrl <- trainControl(method = "cv", number = 10, verboseIter = TRUE, allowParallel = TRUE)
model2 <- train(
factor(FraudFound_P )~ .,
data = training,
trControl = ctrl,
tuneGrid = hyperparams,
method = "xgbTree",
metric = "AUC",
tuneLength = 2, # number of combinations to try in hyperparameter tuning
verbose = TRUE,
objective = "binary:logistic", # specify objective for binary classification
eval_metric = "auc" # specify evaluation metric for binary classification
)
